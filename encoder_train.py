import torch
import torch.nn as nn
import numpy as np
import evaluate
from datasets import Dataset
from transformers import (
    AutoTokenizer,
    TrainingArguments,
    Trainer,
    DataCollatorWithPadding
)
import math
import warnings
warnings.filterwarnings('ignore')

# ----------------------------------------------------
# 1. ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ì„¤ì • (Hugging Face í‘œì¤€ - WordPiece/SentencePiece ê¸°ìˆ )
# ----------------------------------------------------
# Hugging Face í‘œì¤€ Multilingual BERT í† í¬ë‚˜ì´ì €ë¥¼ "tiktokken"ìœ¼ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.
MODEL_NAME = "bert-base-multilingual-cased"
NUM_LABELS = 2
EMBEDDING_DIM = 768
N_HEAD = 12
N_LAYERS = 2
MAX_SEQ_LENGTH = 128

print(f"âœ… í† í¬ë‚˜ì´ì € ë¡œë“œ (Hugging Face í‘œì¤€ 'tiktokken' ê¸°ìˆ ): {MODEL_NAME}")
# Multilingual BERT í† í¬ë‚˜ì´ì € ë¡œë“œ
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)

# ----------------------------------------------------
# 2. ì»¤ìŠ¤í…€ íŠ¸ëœìŠ¤í¬ë¨¸ ì¸ì½”ë” ë¶„ë¥˜ê¸° ì •ì˜ (ìƒˆ ëª¨ë¸)
# ----------------------------------------------------

# íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸ì— í•„ìˆ˜ì ì¸ ìœ„ì¹˜ ì¸ì½”ë”© í´ë˜ìŠ¤
class PositionalEncoding(nn.Module):
    def __init__(self, d_model, max_len=512):
        super().__init__()
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0).transpose(0, 1)
        self.register_buffer('pe', pe)

    def forward(self, x):
        x = x + self.pe[:x.size(0), :]
        return x

class CustomTransformerClassifier(nn.Module):
    def __init__(self, vocab_size, d_model, n_head, n_layers, num_labels, max_len):
        super().__init__()

        self.d_model = d_model

        # 1. ì„ë² ë”© ë ˆì´ì–´ (í† í¬ë‚˜ì´ì €ì˜ ì–´íœ˜ í¬ê¸° ì‚¬ìš©)
        self.embedding = nn.Embedding(vocab_size, d_model, padding_idx=tokenizer.pad_token_id)

        # 2. ìœ„ì¹˜ ì¸ì½”ë”©
        self.pos_encoder = PositionalEncoding(d_model, max_len)

        # 3. íŠ¸ëœìŠ¤í¬ë¨¸ ì¸ì½”ë” ìŠ¤íƒ (ìˆœìˆ˜ PyTorch Transformer Encoder)
        encoder_layer = nn.TransformerEncoderLayer(d_model, n_head, d_model * 4, dropout=0.1, batch_first=False)
        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, n_layers)

        # 4. ë¶„ë¥˜ í—¤ë“œ
        self.classifier = nn.Linear(d_model, num_labels)

    def forward(self, input_ids=None, attention_mask=None, labels=None):
        # 1. ì…ë ¥ í˜•íƒœ ë³€í™˜: (batch_size, seq_len) -> (seq_len, batch_size)
        input_ids = input_ids.transpose(0, 1)

        # 2. ì„ë² ë”© ë° ìœ„ì¹˜ ì¸ì½”ë”© ì¶”ê°€
        embedded = self.embedding(input_ids) * math.sqrt(self.d_model)
        embedded = self.pos_encoder(embedded)

        # 3. ë§ˆìŠ¤í¬ ìƒì„±: attention_mask 0 ìœ„ì¹˜ì— True (íŒ¨ë”© ë¬´ì‹œ)
        src_key_padding_mask = (attention_mask == 0)

        # 4. íŠ¸ëœìŠ¤í¬ë¨¸ ì¸ì½”ë” ì‹¤í–‰
        output = self.transformer_encoder(
            embedded,
            src_key_padding_mask=src_key_padding_mask
        )

        # 5. ë¶„ë¥˜: ì²« ë²ˆì§¸ í† í°([CLS])ì˜ ì¶œë ¥ì„ ë¬¸ì¥ íŠ¹ì§•ìœ¼ë¡œ ì‚¬ìš©
        cls_output = output[0]

        # 6. ìµœì¢… ë¡œì§“ ìƒì„±
        logits = self.classifier(cls_output)

        loss = None
        if labels is not None:
            loss_fct = nn.CrossEntropyLoss()
            loss = loss_fct(logits.view(-1, self.classifier.out_features), labels.view(-1))

        return (loss, logits) if loss is not None else (logits,)

# ì»¤ìŠ¤í…€ ëª¨ë¸ ì´ˆê¸°í™”
model = CustomTransformerClassifier(
    vocab_size=tokenizer.vocab_size,
    d_model=EMBEDDING_DIM,
    n_head=N_HEAD,
    n_layers=N_LAYERS,
    num_labels=NUM_LABELS,
    max_len=MAX_SEQ_LENGTH
)

print("âœ… ì»¤ìŠ¤í…€ íŠ¸ëœìŠ¤í¬ë¨¸ ì¸ì½”ë” ëª¨ë¸ ìƒì„± ì™„ë£Œ.")

# ----------------------------------------------------
# 3. shopping.txt ë°ì´í„°ì…‹ ìƒì„± ë° ì •ì œ
# ----------------------------------------------------
texts = []
labels = []
file_path = "shopping.txt"

print(f"\nğŸ“‚ íŒŒì¼ ë¡œë“œ ë° ì •ì œ ì‹œì‘: {file_path}")

try:
    with open(file_path, 'r', encoding='utf-8') as f:
        for line in f:
            line = line.strip()
            if not line: continue

            parts = line.split('\t', 1)

            if len(parts) == 2:
                try:
                    rating = int(parts[0])
                    text = parts[1]

                    if rating in [1, 2]:
                        label = 0 # ë¶€ì •
                    elif rating in [4, 5]:
                        label = 1 # ê¸ì •
                    else:
                        continue # í‰ì  3ì  ì œì™¸

                    texts.append(text)
                    labels.append(label)

                except ValueError:
                    continue
except FileNotFoundError:
    print(f"âŒ ì˜¤ë¥˜: íŒŒì¼ '{file_path}'ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    exit()

if not texts:
    print("âŒ ì˜¤ë¥˜: ìœ íš¨í•œ ë°ì´í„°ë¥¼ ë¡œë“œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. íŒŒì¼ ë‚´ìš© ë° í˜•ì‹ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
    exit()

raw_dataset = Dataset.from_dict({'text': texts, 'label': labels})
train_test_split = raw_dataset.train_test_split(test_size=0.3, seed=42)
train_dataset = train_test_split['train']
eval_dataset = train_test_split['test']

print(f"ë¡œë“œëœ ì „ì²´ ìƒ˜í”Œ í¬ê¸°: {len(raw_dataset)} (í‰ì  3ì  ì œì™¸)")
print(f"í›ˆë ¨ ë°ì´í„°ì…‹ í¬ê¸°: {len(train_dataset)}")
print(f"í‰ê°€ ë°ì´í„°ì…‹ í¬ê¸°: {len(eval_dataset)}")


# ----------------------------------------------------
# 4. ë°ì´í„° ì „ì²˜ë¦¬ (í‘œì¤€ í† í°í™”)
# ----------------------------------------------------
def tokenize_function(examples):
    # 'tiktokken' í† í¬ë‚˜ì´ì € ê¸°ìˆ ì„ ì‚¬ìš©í•˜ì—¬ ì •ìˆ˜ IDë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    tokenized_inputs = tokenizer(examples["text"], truncation=True, max_length=MAX_SEQ_LENGTH)
    tokenized_inputs["labels"] = examples["label"]
    return tokenized_inputs

tokenized_train_dataset = train_dataset.map(tokenize_function, batched=True, remove_columns=['text'])
tokenized_eval_dataset = eval_dataset.map(tokenize_function, batched=True, remove_columns=['text'])


# ----------------------------------------------------
# 5. í›ˆë ¨ ì„¤ì • ë° Trainer ì‹¤í–‰
# ----------------------------------------------------
accuracy_metric = evaluate.load("accuracy")

def compute_metrics(eval_pred):
    logits, labels = eval_pred
    predictions = np.argmax(logits, axis=-1)
    return accuracy_metric.compute(predictions=predictions, references=labels)

training_args = TrainingArguments(
    output_dir="./custom_standard_transformer_results",
    num_train_epochs=10,
    per_device_train_batch_size=4,
    per_device_eval_batch_size=4,
    learning_rate=1e-4,
    eval_strategy="epoch",
    save_strategy="epoch",
    load_best_model_at_end=True,
    report_to="none",
)

# Data Collator: í† í¬ë‚˜ì´ì €ì— ë§ì¶¤
data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_train_dataset,
    eval_dataset=tokenized_eval_dataset,
    compute_metrics=compute_metrics,
    data_collator=data_collator,
)

print("\nğŸš€ ì»¤ìŠ¤í…€ íŠ¸ëœìŠ¤í¬ë¨¸ ì¸ì½”ë” ë¶„ë¥˜ê¸° í›ˆë ¨ ì‹œì‘ (Hugging Face í‘œì¤€ í† í¬ë‚˜ì´ì € ê¸°ë°˜)...")
trainer.train()

print("\nâœ… í›ˆë ¨ ì™„ë£Œ! ì´ì œ í‘œì¤€ ê¸°ë°˜ ì»¤ìŠ¤í…€ ëª¨ë¸ì´ ì‘ë™í•  ê²ƒì…ë‹ˆë‹¤.")