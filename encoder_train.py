import pandas as pd
from torch.optim import AdamW
from transformers import BertTokenizerFast, BertForSequenceClassification, get_linear_schedule_with_warmup
import torch
from torch.utils.data import TensorDataset, random_split, DataLoader, SequentialSampler, RandomSampler
import numpy as np
import time
import datetime

# 1. ì‚¬ìš©í•  ëª¨ë¸ì˜ ì´ë¦„ ì •ì˜
# BERT ëª¨ë¸ ì¤‘ ë‹¤êµ­ì–´ ì§€ì›, ê¸°ë³¸ ì‚¬ì´ì¦ˆ, ëŒ€ì†Œë¬¸ì êµ¬ë¶„ ë²„ì „ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
MODEL_NAME = "bert-base-multilingual-cased"

# 2. ë‹¤êµ­ì–´ í† í¬ë‚˜ì´ì € ë¡œë“œ
# Fast Tokenizerë¥¼ ì‚¬ìš©í•˜ì—¬ ì†ë„ë¥¼ ê°œì„ í•©ë‹ˆë‹¤.
tokenizer = BertTokenizerFast.from_pretrained(MODEL_NAME)
print(f"âœ… í† í¬ë‚˜ì´ì € ë¡œë“œ ì™„ë£Œ: {MODEL_NAME}")

# 3. ë°ì´í„° ë¡œë”© ë° ë¼ë²¨ë§ í•¨ìˆ˜ (ì´ì „ê³¼ ë™ì¼)
def load_and_preprocess_data(file_path):
    """
    íŒŒì¼ì„ ì½ê³ , í‰ì ì„ ê¸°ë°˜ìœ¼ë¡œ ê¸ì •/ë¶€ì • ë¼ë²¨ì„ ë¶€ì—¬í•˜ë©°, 3ì  ë¦¬ë·°ë¥¼ ì œì™¸í•©ë‹ˆë‹¤.
    """
    data = []
    # íŒŒì¼ ì½ê¸°: í‰ì ê³¼ ë¦¬ë·° ë‚´ìš©ì´ íƒ­(\t)ìœ¼ë¡œ êµ¬ë¶„ë˜ì–´ ìˆìŒ
    with open(file_path, 'r', encoding='utf-8') as f:
        for line in f:
            line = line.strip()
            if not line:
                continue

            parts = line.split('\t', 1)
            if len(parts) == 2:
                try:
                    score = int(parts[0])
                    review = parts[1]

                    # 3ì  ì œì™¸
                    if score == 3:
                        continue

                    # ê¸ì •(1): 4~5ì , ë¶€ì •(0): 1~2ì 
                    label = 1 if score >= 4 else 0
                    data.append([review, label])

                except ValueError:
                    continue

    df = pd.DataFrame(data, columns=['text', 'label'])
    print(f"âœ… ì´ {len(df)}ê°œì˜ ë¦¬ë·° ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ (3ì  ì œì™¸).")
    return df

# 4. í† í°í™” ì‹¤í–‰ í•¨ìˆ˜
def tokenize_data(df, tokenizer, max_length=128):
    """
    DataFrameì˜ í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ì§€ì •ëœ ë‹¤êµ­ì–´ í† í¬ë‚˜ì´ì €ë¥¼ ì‚¬ìš©í•˜ì—¬ í† í°í™”í•©ë‹ˆë‹¤.
    """
    # Hugging Face í† í¬ë‚˜ì´ì €ëŠ” ì¸ì½”ë”© ê³¼ì •ì—ì„œ í† í°í™”, ì¸ë±ìŠ¤ ë³€í™˜, íŒ¨ë”©, ì–´í…ì…˜ ë§ˆìŠ¤í¬ ìƒì„±ì„ ëª¨ë‘ ì²˜ë¦¬í•©ë‹ˆë‹¤.
    tokenized_data = tokenizer(
        df['text'].tolist(),
        truncation=True,
        padding='max_length',
        max_length=max_length,
        return_tensors='pt' # PyTorch í…ì„œ í˜•íƒœë¡œ ë°˜í™˜
    )
    return tokenized_data, df['label'].tolist()


# --- ì‹¤í–‰ ---
file_path = 'shopping.txt' # íŒŒì¼ ê²½ë¡œ ì§€ì • (ì‹¤ì œ íŒŒì¼ì´ í˜„ì¬ ê²½ë¡œì— ìˆë‹¤ê³  ê°€ì •)

# 1. ë°ì´í„° ë¡œë”© ë° ë¼ë²¨ë§
df_data = load_and_preprocess_data(file_path)

# 2. í† í°í™” ì‹¤í–‰
tokenized_inputs, labels = tokenize_data(df_data, tokenizer, max_length=128)

# ê²°ê³¼ í™•ì¸ (ì²« ë²ˆì§¸ ë¦¬ë·°)
print("\n--- í† í°í™” ê²°ê³¼ (ì²« ë²ˆì§¸ ë¦¬ë·°) ---")
print(f"ì›ë¬¸: {df_data.iloc[0]['text']}")
print(f"ë¼ë²¨: {df_data.iloc[0]['label']} (1:ê¸ì •, 0:ë¶€ì •)")
print("Input IDs (í† í° ì¸ë±ìŠ¤):", tokenized_inputs['input_ids'][0][:15]) # ì• 15ê°œ ì¶œë ¥
print("Attention Mask:", tokenized_inputs['attention_mask'][0][:15])
print("Token Type IDs:", tokenized_inputs['token_type_ids'][0][:15])

print(f"\nâœ… ìµœì¢… ì¤€ë¹„ëœ ë°ì´í„° ê°œìˆ˜: {len(labels)}")


# --- 3. ë°ì´í„°ì…‹ ë¶„í•  ë° ë°ì´í„°ë¡œë” ìƒì„± ---

# PyTorch í…ì„œ í˜•íƒœë¡œ ë³€í™˜ëœ í† í°í™” ê²°ê³¼ì™€ ë¼ë²¨
input_ids = tokenized_inputs['input_ids']
attention_masks = tokenized_inputs['attention_mask']
token_type_ids = tokenized_inputs['token_type_ids']
labels_tensor = torch.tensor(labels)

# 1. TensorDataset ìƒì„±
# BERT ì…ë ¥ì— í•„ìš”í•œ ëª¨ë“  í…ì„œë¥¼ í•˜ë‚˜ì˜ ë°ì´í„°ì…‹ìœ¼ë¡œ ë¬¶ìŠµë‹ˆë‹¤.
dataset = TensorDataset(input_ids, attention_masks, token_type_ids, labels_tensor)

# 2. í›ˆë ¨, ê²€ì¦, í…ŒìŠ¤íŠ¸ ì…‹ í¬ê¸° ê³„ì‚° (ì˜ˆ: 80% / 10% / 10%)
train_size = int(0.8 * len(dataset))
val_size = int(0.1 * len(dataset))
test_size = len(dataset) - train_size - val_size

# 3. random_splitì„ ì‚¬ìš©í•˜ì—¬ ë°ì´í„°ì…‹ ë¶„í• 
train_dataset, val_dataset, test_dataset = random_split(
    dataset, [train_size, val_size, test_size]
)

print(f"\n--- ë°ì´í„°ì…‹ ë¶„í•  ê²°ê³¼ ---")
print(f"ì´ ë°ì´í„° ìˆ˜: {len(dataset)}")
print(f"í›ˆë ¨ ì…‹ (Train Set) í¬ê¸°: {len(train_dataset)}")
print(f"ê²€ì¦ ì…‹ (Validation Set) í¬ê¸°: {len(val_dataset)}")
print(f"í…ŒìŠ¤íŠ¸ ì…‹ (Test Set) í¬ê¸°: {len(test_dataset)}")

# 4. DataLoader ìƒì„± (ë°°ì¹˜ í•™ìŠµ ì¤€ë¹„)
batch_size = 16 

# í›ˆë ¨ ë°ì´í„°ë¡œë”: ë¬´ì‘ìœ„ ìƒ˜í”Œë§
train_dataloader = DataLoader(
    train_dataset,
    sampler=RandomSampler(train_dataset), # ë°ì´í„°ë¥¼ ë¬´ì‘ìœ„ë¡œ ì„ìŒ
    batch_size=batch_size
)

# ê²€ì¦ ë° í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¡œë”: ìˆœì°¨ì  ìƒ˜í”Œë§
val_dataloader = DataLoader(
    val_dataset,
    sampler=SequentialSampler(val_dataset), # ìˆœì„œëŒ€ë¡œ ìƒ˜í”Œë§
    batch_size=batch_size
)

test_dataloader = DataLoader(
    test_dataset,
    sampler=SequentialSampler(test_dataset), # ìˆœì„œëŒ€ë¡œ ìƒ˜í”Œë§
    batch_size=batch_size
)

print(f"âœ… DataLoader ìƒì„± ì™„ë£Œ (Batch Size: {batch_size}).")


# --- 4. BERT ëª¨ë¸ ë¡œë“œ ë° ì„¤ì • ---

# 1. Device ì„¤ì • (GPU ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸)
if torch.cuda.is_available():    
    device = torch.device("cuda")
    print('âœ… GPU ì‚¬ìš© ê°€ëŠ¥: {}'.format(torch.cuda.get_device_name(0)))
else:
    device = torch.device("cpu")
    print('âš ï¸ GPUë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. CPUë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.')

# 2. ëª¨ë¸ ë¡œë“œ
# ë¶„ë¥˜ íƒœìŠ¤í¬ë¥¼ ìœ„í•´ BertForSequenceClassificationì„ ì‚¬ìš©í•˜ë©°, í´ë˜ìŠ¤ ê°œìˆ˜(2ê°œ: ê¸ì •/ë¶€ì •)ë¥¼ ì§€ì •í•©ë‹ˆë‹¤.
model = BertForSequenceClassification.from_pretrained(
    MODEL_NAME, 
    num_labels = 2,    # ì¶œë ¥ í´ë˜ìŠ¤ ê°œìˆ˜ (ê¸ì •, ë¶€ì •)
    output_attentions = False, # Attention ê°€ì¤‘ì¹˜ ë°˜í™˜ ì•ˆ í•¨
    output_hidden_states = False, # ëª¨ë“  hidden state ë°˜í™˜ ì•ˆ í•¨
)

# ëª¨ë¸ì„ ì„¤ì •ëœ Deviceë¡œ ì´ë™
model.to(device)

print(f"âœ… BERT ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {MODEL_NAME} (num_labels=2)")


# 3. ì˜µí‹°ë§ˆì´ì € ë° í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬ ì„¤ì •
# BERT Fine-tuningì— ì¼ë°˜ì ìœ¼ë¡œ ì‚¬ìš©ë˜ëŠ” í•˜ì´í¼íŒŒë¼ë¯¸í„°
epochs = 4 # í•™ìŠµ ì—í­ ìˆ˜ (ê¶Œì¥: 2~4)
learning_rate = 2e-5 # BERT Fine-tuningì— ì í•©í•œ ì‘ì€ í•™ìŠµë¥  (ê¶Œì¥: 1e-5 ~ 5e-5)
adam_epsilon = 1e-8 
warmup_steps = 0 

# ì˜µí‹°ë§ˆì´ì € ì„¤ì • (AdamW: ê°€ì¤‘ì¹˜ ê°ì‡ (Weight Decay)ê°€ ê°œì„ ëœ Adam)
optimizer = AdamW(
    model.parameters(),
    lr = learning_rate,
    eps = adam_epsilon
)

# í•™ìŠµ ìŠ¤ì¼€ì¤„ëŸ¬ ì„¤ì • (Linear Warmup and Decay)
total_steps = len(train_dataloader) * epochs
scheduler = get_linear_schedule_with_warmup(
    optimizer, 
    num_warmup_steps = warmup_steps, 
    num_training_steps = total_steps
)

print(f"âœ… ì˜µí‹°ë§ˆì´ì € ë° ìŠ¤ì¼€ì¤„ëŸ¬ ì„¤ì • ì™„ë£Œ (í•™ìŠµë¥ : {learning_rate}, ì—í­: {epochs})")
print("ì´ì œ í•™ìŠµ ë£¨í”„(Training Loop)ë¥¼ ì¶”ê°€í•˜ì—¬ Fine-tuningì„ ì§„í–‰í•˜ì‹œë©´ ë©ë‹ˆë‹¤.")



# --- 5. í•™ìŠµ ë£¨í”„ (Fine-tuning) ë° í‰ê°€ ---

# ì •í™•ë„ ê³„ì‚° í•¨ìˆ˜ ì •ì˜
def flat_accuracy(preds, labels):
    """ì˜ˆì¸¡ ê²°ê³¼ì™€ ì‹¤ì œ ë¼ë²¨ì„ ë¹„êµí•˜ì—¬ ì •í™•ë„ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤."""
    pred_flat = np.argmax(preds, axis=1).flatten()
    labels_flat = labels.flatten()
    return np.sum(pred_flat == labels_flat) / len(labels_flat)

# ì‹œê°„ í¬ë§·íŒ… í•¨ìˆ˜
def format_time(elapsed):
    """ì‹œê°„ì„ HH:MM:SS í˜•íƒœë¡œ í¬ë§·íŒ…í•©ë‹ˆë‹¤."""
    elapsed_rounded = int(round((elapsed)))
    return str(datetime.timedelta(seconds=elapsed_rounded))

# í•™ìŠµ ì¤€ë¹„
training_stats = []
total_t0 = time.time()

# ëª¨ë¸ì„ í›ˆë ¨ ëª¨ë“œë¡œ ì„¤ì •
model.zero_grad()
model.train()

print("\n\n--- ğŸš€ BERT Fine-tuning ì‹œì‘ ---")

for epoch_i in range(0, epochs):
    
    # ========================================
    #               í›ˆë ¨ (Training)
    # ========================================

    print(f'\n======== Epoch {epoch_i + 1} / {epochs} ========')
    print('Training...')

    t0 = time.time()
    total_train_loss = 0

    for step, batch in enumerate(train_dataloader):
        # 1. ë°°ì¹˜ ë°ì´í„° Deviceë¡œ ì´ë™
        b_input_ids = batch[0].to(device)
        b_input_mask = batch[1].to(device)
        b_token_type_ids = batch[2].to(device)
        b_labels = batch[3].to(device)
        
        # 2. ëª¨ë¸ì— ì…ë ¥
        # forward() ì‹¤í–‰ ì‹œ, labelsë¥¼ ì¸ìë¡œ ì œê³µí•˜ë©´ lossë¥¼ ê³„ì‚°í•´ ë°˜í™˜í•¨
        outputs = model(b_input_ids, 
                        token_type_ids=b_token_type_ids, 
                        attention_mask=b_input_mask, 
                        labels=b_labels)
        
        loss = outputs.loss
        total_train_loss += loss.item()

        # 3. ì—­ì „íŒŒ ë° ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸
        loss.backward()
        
        # í´ë¦¬í•‘(Clipping)ì„ í†µí•´ ê¸°ìš¸ê¸°ê°€ ë„ˆë¬´ ì»¤ì§€ëŠ” ê²ƒì„ ë°©ì§€
        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)

        # ì˜µí‹°ë§ˆì´ì €ë¡œ íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸
        optimizer.step()

        # í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬ ì—…ë°ì´íŠ¸
        scheduler.step()
        
        # ê¸°ìš¸ê¸° ì´ˆê¸°í™”
        model.zero_grad()
        
        if step % 50 == 0 and not step == 0:
            elapsed = format_time(time.time() - t0)
            print(f'  Batch {step:>5,} of {len(train_dataloader):>5,}. Loss: {loss.item():.2f}. Elapsed: {elapsed}.')

    avg_train_loss = total_train_loss / len(train_dataloader)           
    training_time = format_time(time.time() - t0)

    print(f'\n  í‰ê·  í›ˆë ¨ ì†ì‹¤: {avg_train_loss:.2f}')
    print(f'  í›ˆë ¨ ì™„ë£Œ ì‹œê°„: {training_time}')


    # ========================================
    #             ê²€ì¦ (Validation)
    # ========================================
    
    print('\nRunning Validation...')

    t0 = time.time()
    model.eval() # ëª¨ë¸ì„ í‰ê°€ ëª¨ë“œë¡œ ì„¤ì • (ë“œë¡­ì•„ì›ƒ ë“±ì´ ë¹„í™œì„±í™”ë¨)

    total_eval_accuracy = 0
    total_eval_loss = 0
    nb_eval_steps = 0

    for batch in val_dataloader:
        
        b_input_ids = batch[0].to(device)
        b_input_mask = batch[1].to(device)
        b_token_type_ids = batch[2].to(device)
        b_labels = batch[3].to(device)
        
        with torch.no_grad(): # ê¸°ìš¸ê¸° ê³„ì‚° ë¹„í™œì„±í™” (ë©”ëª¨ë¦¬ ì ˆì•½)
            outputs = model(b_input_ids, 
                            token_type_ids=b_token_type_ids, 
                            attention_mask=b_input_mask,
                            labels=b_labels)
            
        loss = outputs.loss
        logits = outputs.logits # ì˜ˆì¸¡ ê²°ê³¼

        total_eval_loss += loss.item()
        
        # ì •í™•ë„ ê³„ì‚°
        logits = logits.detach().cpu().numpy()
        label_ids = b_labels.to('cpu').numpy()
        total_eval_accuracy += flat_accuracy(logits, label_ids)
        
    avg_val_accuracy = total_eval_accuracy / len(val_dataloader)
    print(f'  ì •í™•ë„: {avg_val_accuracy:.4f}')

    avg_val_loss = total_eval_loss / len(val_dataloader)
    validation_time = format_time(time.time() - t0)
    
    print(f'  ê²€ì¦ ì†ì‹¤: {avg_val_loss:.2f}')
    print(f'  ê²€ì¦ ì™„ë£Œ ì‹œê°„: {validation_time}')

    # ì—í­ë³„ ê²°ê³¼ ì €ì¥
    training_stats.append(
        {
            'epoch': epoch_i + 1,
            'Training Loss': avg_train_loss,
            'Valid. Loss': avg_val_loss,
            'Valid. Accur.': avg_val_accuracy,
            'Training Time': training_time,
            'Validation Time': validation_time
        }
    )

print('\n\n--- âœ… Fine-tuning ì™„ë£Œ ---')
print(f'ì „ì²´ í•™ìŠµ ì†Œìš” ì‹œê°„: {format_time(time.time()-total_t0)}')


# --- 6. ëª¨ë¸ ì €ì¥ ---

import os

# ëª¨ë¸ ë° í† í¬ë‚˜ì´ì €ë¥¼ ì €ì¥í•  ë””ë ‰í† ë¦¬ ê²½ë¡œ ì§€ì •
output_dir = './model_save/'

# ë””ë ‰í† ë¦¬ ìƒì„±
if not os.path.exists(output_dir):
    os.makedirs(output_dir)
    print(f"\nâœ… ëª¨ë¸ ì €ì¥ ë””ë ‰í† ë¦¬ ìƒì„±: {output_dir}")

# 1. ëª¨ë¸ ì €ì¥
print("Saving model to %s" % output_dir)
model_to_save = model.module if hasattr(model, 'module') else model  # ë°ì´í„° ë³‘ë ¬í™” ì²˜ë¦¬
model_to_save.save_pretrained(output_dir)

# 2. í† í¬ë‚˜ì´ì € ì €ì¥
tokenizer.save_pretrained(output_dir)

print("âœ… ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ì €ì¥ ì™„ë£Œ.")