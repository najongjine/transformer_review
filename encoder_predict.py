import torch
import numpy as np
from transformers import BertTokenizerFast, BertForSequenceClassification
import time

# --- 1. ì €ì¥ëœ ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ ê²½ë¡œ ì„¤ì • ---
# í•™ìŠµ ì½”ë“œì—ì„œ ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ê°€ ì €ì¥ëœ ë””ë ‰í† ë¦¬ì…ë‹ˆë‹¤.
MODEL_DIR = './model_save/' 

# --- 2. Device ì„¤ì • ---
# í•™ìŠµ ë•Œì™€ ë™ì¼í•˜ê²Œ GPU ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ë¥¼ í™•ì¸í•©ë‹ˆë‹¤.
if torch.cuda.is_available():    
    device = torch.device("cuda")
    print('âœ… GPU ì‚¬ìš© ê°€ëŠ¥: {}'.format(torch.cuda.get_device_name(0)))
else:
    device = torch.device("cpu")
    print('âš ï¸ GPUë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. CPUë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.')

# --- 3. ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ ---
try:
    # 1. í† í¬ë‚˜ì´ì € ë¡œë“œ (í•™ìŠµ ì‹œ ì‚¬ìš©í•œ BertTokenizerFast)
    tokenizer = BertTokenizerFast.from_pretrained(MODEL_DIR)
    print(f"âœ… í† í¬ë‚˜ì´ì € ë¡œë“œ ì™„ë£Œ: {MODEL_DIR}")

    # 2. ëª¨ë¸ ë¡œë“œ (í•™ìŠµëœ ê°€ì¤‘ì¹˜ë¥¼ í¬í•¨í•˜ëŠ” BertForSequenceClassification)
    model = BertForSequenceClassification.from_pretrained(MODEL_DIR)
    model.to(device)
    model.eval() # ëª¨ë¸ì„ í‰ê°€ ëª¨ë“œë¡œ ì„¤ì •
    print(f"âœ… BERT ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {MODEL_DIR}")

except Exception as e:
    print(f"âŒ ëª¨ë¸/í† í¬ë‚˜ì´ì € ë¡œë“œ ì‹¤íŒ¨: {e}")
    print("   'model_save' ë””ë ‰í† ë¦¬ì— config.json, model.safetensors ë“±ì´ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.")
    exit()

# --- 4. ì¶”ë¡ (Inference) í•¨ìˆ˜ ì •ì˜ ---

def predict_sentiment(text: str, max_length: int = 128):
    """
    ì£¼ì–´ì§„ í…ìŠ¤íŠ¸ì— ëŒ€í•´ ê°ì„± ë¶„ì„(ê¸ì •/ë¶€ì •)ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
    
    :param text: ë¶„ë¥˜í•  ì…ë ¥ í…ìŠ¤íŠ¸
    :param max_length: í† í°í™” ì‹œ ìµœëŒ€ ê¸¸ì´
    :return: (ì˜ˆì¸¡ ë¼ë²¨ (1: ê¸ì •, 0: ë¶€ì •), ê¸ì • í™•ë¥ , ë¶€ì • í™•ë¥ )
    """
    
    # 1. ì…ë ¥ í…ìŠ¤íŠ¸ í† í°í™”
    # PyTorch í…ì„œ í˜•íƒœë¡œ ë°˜í™˜
    encoded_input = tokenizer(
        text,
        truncation=True,
        padding='max_length',
        max_length=max_length,
        return_tensors='pt' 
    )

    # 2. ì…ë ¥ ë°ì´í„°ë¥¼ Deviceë¡œ ì´ë™
    input_ids = encoded_input['input_ids'].to(device)
    attention_mask = encoded_input['attention_mask'].to(device)
    token_type_ids = encoded_input['token_type_ids'].to(device)
    
    # 3. ëª¨ë¸ ì¶”ë¡  ì‹¤í–‰
    with torch.no_grad(): # í‰ê°€ ëª¨ë“œì´ë¯€ë¡œ ê¸°ìš¸ê¸° ê³„ì‚° ë¹„í™œì„±í™”
        outputs = model(
            input_ids, 
            token_type_ids=token_type_ids, 
            attention_mask=attention_mask
        )
    
    # 4. ì˜ˆì¸¡ ê²°ê³¼ (Logits) ì²˜ë¦¬
    logits = outputs.logits # Logits: ë¶„ë¥˜ ì „ì˜ ì›ì‹œ ì ìˆ˜ í…ì„œ
    
    # Logitsì„ í™•ë¥ ë¡œ ë³€í™˜ (Softmax ì‚¬ìš©)
    probabilities = torch.softmax(logits, dim=1)
    
    # ê²°ê³¼ë¥¼ CPUë¡œ ì´ë™í•˜ê³  NumPy ë°°ì—´ë¡œ ë³€í™˜
    prob_np = probabilities.cpu().numpy()[0]
    
    # ì˜ˆì¸¡ ë¼ë²¨ (ì ìˆ˜ê°€ ê°€ì¥ ë†’ì€ ì¸ë±ìŠ¤)
    predicted_label = np.argmax(prob_np) 
    
    # ê¸ì •(Positive, ì¸ë±ìŠ¤ 1) í™•ë¥ , ë¶€ì •(Negative, ì¸ë±ìŠ¤ 0) í™•ë¥ 
    # í•™ìŠµ ì‹œ ë¼ë²¨ë§: ê¸ì •(1): 4~5ì , ë¶€ì •(0): 1~2ì 
    neg_prob = prob_np[0]
    pos_prob = prob_np[1]
    
    return predicted_label.item(), pos_prob.item(), neg_prob.item()

# --- 5. í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ---
if __name__ == "__main__":
    
    print("\n--- ğŸ§  ê°ì„± ë¶„ë¥˜ í…ŒìŠ¤íŠ¸ ---")
    
    test_texts = [
        "ëŠê·¸ì§‘ ëˆ„ë ì´ë„ ê±°ë¥¼ë“¯.", # ë¶€ì •
        "ëŠê¸ˆë§ˆ ë§Œìˆ˜ë¬´ê°•",
        "ëŠê¸ˆë§ˆ",
        "Justine Beaver might like it" # ë‹¤êµ­ì–´ í…ŒìŠ¤íŠ¸
    ]
    
    for text in test_texts:
        t_start = time.time()
        label, pos_prob, neg_prob = predict_sentiment(text)
        t_end = time.time()

        sentiment = "ê¸ì • (Positive)" if label == 1 else "ë¶€ì • (Negative)"
        
        print(f"\n[ì…ë ¥]: {text}")
        print(f"  [ê²°ê³¼]: {sentiment}")
        print(f"  [í™•ë¥ ]: ê¸ì • {pos_prob:.4f} | ë¶€ì • {neg_prob:.4f}")
        print(f"  [ì‹œê°„]: {(t_end - t_start) * 1000:.2f} ms")